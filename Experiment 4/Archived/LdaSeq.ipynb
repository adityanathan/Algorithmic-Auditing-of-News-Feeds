{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import timeit\n",
    "import spacy\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaMulticore, ldaseqmodel\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.matutils import hellinger\n",
    "import helper as he\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading our corpus and dictionary\n",
    "# try:\n",
    "#     dictionary = Dictionary.load('datasets/news_dictionary')\n",
    "# except FileNotFoundError as e:\n",
    "#     raise ValueError(\"SKIP: Please download the Corpus/news_dictionary dataset.\")\n",
    "# corpus = bleicorpus.BleiCorpus('datasets/news_corpus')\n",
    "# # it's very important that your corpus is saved in order of your time-slices!\n",
    "\n",
    "# time_slice = [438, 430, 456]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/preprocessed_data/aadhar_corp.pkl','rb') as f:\n",
    "    data_lemmatized, id2word, corpus = pickle.load(f)\n",
    "    \n",
    "with open('../data/massmedia-data/aadhar_date_indexed.pkl','rb') as f:\n",
    "    _, time_index_arr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-04 20:14:53,878 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-12-04 20:14:57,865 : INFO : adding document #10000 to Dictionary(53262 unique tokens: ['aadhaar', 'access', 'adilabad', 'also', 'anand']...)\n",
      "2019-12-04 20:15:00,046 : INFO : built Dictionary(65957 unique tokens: ['aadhaar', 'access', 'adilabad', 'also', 'anand']...) from 13908 documents (total 2792599 corpus positions)\n",
      "2019-12-04 20:15:00,246 : INFO : discarding 65857 tokens: [('aadhaar', 11563), ('adilabad', 44), ('also', 8094), ('anand', 119), ('anukunta', 1), ('attend', 554), ('basis', 1004), ('become', 1791), ('bring', 2087), ('card', 8578)]...\n",
      "2019-12-04 20:15:00,247 : INFO : keeping 100 tokens which were in no less than 5 and no more than 1390 (=10.0%) documents\n",
      "2019-12-04 20:15:00,304 : INFO : resulting dictionary: Dictionary(100 unique tokens: ['access', 'holder', 'other', 'special', 'village']...)\n"
     ]
    }
   ],
   "source": [
    "id2word = Dictionary(data_lemmatized)\n",
    "id2word.filter_extremes(no_below=5, no_above=0.1, keep_n=100, keep_tokens=None)\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in data_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeslice_data(index_arr):\n",
    "    arr = []\n",
    "    for i in index_arr.values():\n",
    "        arr.append(i)\n",
    "        \n",
    "    diff_arr=[]\n",
    "    for i in range(1,len(arr)):\n",
    "        diff_arr.append(arr[i]-arr[i-1])\n",
    "    return diff_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_slices = generate_timeslice_data(time_index_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-04 20:15:02,249 : INFO : using symmetric eta at 0.02857142857142857\n",
      "2019-12-04 20:15:02,251 : INFO : using serial LDA version on this node\n",
      "2019-12-04 20:15:02,258 : INFO : running online (single-pass) LDA training, 35 topics, 1 passes over the supplied corpus of 13908 documents, updating model once every 2000 documents, evaluating perplexity every 13908 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-12-04 20:15:02,260 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2019-12-04 20:15:02,266 : INFO : PROGRESS: pass 0, at document #2000/13908\n",
      "2019-12-04 20:15:03,635 : INFO : merging changes from 2000 documents into a model of 13908 documents\n",
      "2019-12-04 20:15:03,652 : INFO : topic #23 (0.010): 0.064*\"consumer\" + 0.031*\"meet\" + 0.031*\"apply\" + 0.030*\"customer\" + 0.027*\"clear\" + 0.026*\"june\" + 0.022*\"meeting\" + 0.022*\"total\" + 0.021*\"election\" + 0.021*\"education\"\n",
      "2019-12-04 20:15:03,655 : INFO : topic #16 (0.010): 0.087*\"bill\" + 0.034*\"food\" + 0.033*\"sector\" + 0.032*\"poor\" + 0.026*\"committee\" + 0.022*\"director\" + 0.022*\"increase\" + 0.022*\"fund\" + 0.021*\"finance\" + 0.021*\"head\"\n",
      "2019-12-04 20:15:03,656 : INFO : topic #28 (0.010): 0.039*\"poor\" + 0.034*\"continue\" + 0.031*\"population\" + 0.031*\"finance\" + 0.031*\"cost\" + 0.027*\"demand\" + 0.025*\"meet\" + 0.024*\"unique\" + 0.023*\"committee\" + 0.022*\"meeting\"\n",
      "2019-12-04 20:15:03,667 : INFO : topic #30 (0.010): 0.091*\"woman\" + 0.052*\"increase\" + 0.040*\"begin\" + 0.038*\"cost\" + 0.038*\"administration\" + 0.021*\"december\" + 0.021*\"access\" + 0.019*\"identification\" + 0.019*\"still\" + 0.018*\"release\"\n",
      "2019-12-04 20:15:03,680 : INFO : topic #20 (0.010): 0.084*\"committee\" + 0.055*\"customer\" + 0.043*\"database\" + 0.042*\"unique\" + 0.034*\"expect\" + 0.026*\"begin\" + 0.025*\"election\" + 0.023*\"announce\" + 0.023*\"administration\" + 0.022*\"verification\"\n",
      "2019-12-04 20:15:03,681 : INFO : topic diff=1.650632, rho=1.000000\n",
      "2019-12-04 20:15:03,685 : INFO : PROGRESS: pass 0, at document #4000/13908\n",
      "2019-12-04 20:15:05,238 : INFO : merging changes from 2000 documents into a model of 13908 documents\n",
      "2019-12-04 20:15:05,247 : INFO : topic #32 (0.010): 0.139*\"village\" + 0.084*\"release\" + 0.072*\"administration\" + 0.030*\"group\" + 0.029*\"local\" + 0.028*\"meeting\" + 0.027*\"per_cent\" + 0.024*\"special\" + 0.023*\"population\" + 0.021*\"apply\"\n",
      "2019-12-04 20:15:05,252 : INFO : topic #7 (0.010): 0.092*\"december\" + 0.049*\"head\" + 0.048*\"today\" + 0.046*\"director\" + 0.037*\"local\" + 0.037*\"unique\" + 0.033*\"digital\" + 0.024*\"speak\" + 0.024*\"other\" + 0.024*\"special\"\n",
      "2019-12-04 20:15:05,259 : INFO : topic #4 (0.010): 0.119*\"food\" + 0.056*\"woman\" + 0.048*\"pension\" + 0.034*\"holder\" + 0.028*\"education\" + 0.028*\"finance\" + 0.026*\"director\" + 0.023*\"implementation\" + 0.021*\"aim\" + 0.021*\"poor\"\n",
      "2019-12-04 20:15:05,273 : INFO : topic #14 (0.010): 0.173*\"power\" + 0.044*\"demand\" + 0.040*\"look\" + 0.035*\"continue\" + 0.030*\"back\" + 0.029*\"consumer\" + 0.029*\"prime_minister\" + 0.028*\"finance\" + 0.026*\"cost\" + 0.021*\"thursday\"\n",
      "2019-12-04 20:15:05,293 : INFO : topic #11 (0.010): 0.136*\"party\" + 0.134*\"election\" + 0.056*\"meeting\" + 0.040*\"special\" + 0.024*\"meet\" + 0.019*\"remain\" + 0.019*\"speak\" + 0.018*\"thursday\" + 0.015*\"live\" + 0.015*\"support\"\n",
      "2019-12-04 20:15:05,295 : INFO : topic diff=0.470085, rho=0.707107\n",
      "2019-12-04 20:15:05,312 : INFO : PROGRESS: pass 0, at document #6000/13908\n",
      "2019-12-04 20:15:06,675 : INFO : merging changes from 2000 documents into a model of 13908 documents\n",
      "2019-12-04 20:15:06,685 : INFO : topic #4 (0.010): 0.172*\"food\" + 0.062*\"woman\" + 0.040*\"holder\" + 0.037*\"pension\" + 0.031*\"director\" + 0.030*\"implementation\" + 0.028*\"aim\" + 0.027*\"poor\" + 0.026*\"able\" + 0.025*\"finance\"\n",
      "2019-12-04 20:15:06,687 : INFO : topic #12 (0.010): 0.081*\"return\" + 0.056*\"list\" + 0.049*\"live\" + 0.045*\"long\" + 0.043*\"sector\" + 0.042*\"raise\" + 0.040*\"increase\" + 0.039*\"back\" + 0.034*\"cost\" + 0.028*\"pass\"\n",
      "2019-12-04 20:15:06,691 : INFO : topic #18 (0.010): 0.083*\"food\" + 0.080*\"per_cent\" + 0.074*\"bill\" + 0.054*\"population\" + 0.047*\"share\" + 0.044*\"target\" + 0.030*\"remain\" + 0.021*\"finance\" + 0.020*\"big\" + 0.020*\"poor\"\n",
      "2019-12-04 20:15:06,698 : INFO : topic #1 (0.010): 0.096*\"poor\" + 0.091*\"amount\" + 0.061*\"apply\" + 0.052*\"total\" + 0.041*\"transaction\" + 0.040*\"step\" + 0.038*\"consumer\" + 0.037*\"per_cent\" + 0.024*\"target\" + 0.023*\"sector\"\n",
      "2019-12-04 20:15:06,700 : INFO : topic #26 (0.010): 0.104*\"group\" + 0.095*\"sector\" + 0.051*\"exist\" + 0.038*\"finance\" + 0.034*\"introduce\" + 0.030*\"per_cent\" + 0.029*\"education\" + 0.024*\"section\" + 0.020*\"step\" + 0.020*\"increase\"\n",
      "2019-12-04 20:15:06,706 : INFO : topic diff=0.393104, rho=0.577350\n",
      "2019-12-04 20:15:06,715 : INFO : PROGRESS: pass 0, at document #8000/13908\n",
      "2019-12-04 20:15:07,921 : INFO : merging changes from 2000 documents into a model of 13908 documents\n",
      "2019-12-04 20:15:07,926 : INFO : topic #11 (0.010): 0.208*\"election\" + 0.156*\"party\" + 0.067*\"meeting\" + 0.046*\"special\" + 0.025*\"meet\" + 0.022*\"support\" + 0.018*\"thursday\" + 0.016*\"remain\" + 0.015*\"live\" + 0.015*\"speak\"\n",
      "2019-12-04 20:15:07,931 : INFO : topic #10 (0.010): 0.237*\"list\" + 0.084*\"tuesday\" + 0.055*\"total\" + 0.043*\"age\" + 0.037*\"expect\" + 0.034*\"result\" + 0.032*\"fail\" + 0.029*\"photo\" + 0.026*\"election\" + 0.023*\"poor\"\n",
      "2019-12-04 20:15:07,933 : INFO : topic #33 (0.010): 0.267*\"complaint\" + 0.129*\"holder\" + 0.057*\"charge\" + 0.035*\"action\" + 0.028*\"speak\" + 0.025*\"administration\" + 0.023*\"fail\" + 0.022*\"wednesday\" + 0.022*\"regard\" + 0.018*\"thursday\"\n",
      "2019-12-04 20:15:07,936 : INFO : topic #16 (0.010): 0.424*\"bill\" + 0.044*\"pass\" + 0.032*\"finance\" + 0.027*\"house\" + 0.025*\"privacy\" + 0.023*\"introduce\" + 0.018*\"power\" + 0.017*\"committee\" + 0.013*\"raise\" + 0.013*\"poor\"\n",
      "2019-12-04 20:15:07,940 : INFO : topic #25 (0.010): 0.176*\"pass\" + 0.096*\"bill\" + 0.089*\"request\" + 0.051*\"introduce\" + 0.039*\"matter\" + 0.026*\"list\" + 0.021*\"action\" + 0.018*\"section\" + 0.016*\"extend\" + 0.016*\"administration\"\n",
      "2019-12-04 20:15:07,952 : INFO : topic diff=0.350254, rho=0.500000\n",
      "2019-12-04 20:15:07,955 : INFO : PROGRESS: pass 0, at document #10000/13908\n",
      "2019-12-04 20:15:09,081 : INFO : merging changes from 2000 documents into a model of 13908 documents\n",
      "2019-12-04 20:15:09,094 : INFO : topic #9 (0.010): 0.210*\"today\" + 0.044*\"approach\" + 0.042*\"matter\" + 0.040*\"later\" + 0.040*\"challenge\" + 0.039*\"hand\" + 0.038*\"thursday\" + 0.033*\"section\" + 0.022*\"other\" + 0.022*\"clear\"\n",
      "2019-12-04 20:15:09,096 : INFO : topic #21 (0.010): 0.219*\"age\" + 0.127*\"unique\" + 0.120*\"database\" + 0.026*\"hand\" + 0.023*\"begin\" + 0.022*\"later\" + 0.021*\"other\" + 0.021*\"photo\" + 0.019*\"sector\" + 0.019*\"pension\"\n",
      "2019-12-04 20:15:09,099 : INFO : topic #5 (0.010): 0.374*\"education\" + 0.056*\"meeting\" + 0.041*\"meet\" + 0.020*\"director\" + 0.018*\"june\" + 0.017*\"able\" + 0.015*\"step\" + 0.013*\"big\" + 0.013*\"long\" + 0.012*\"group\"\n",
      "2019-12-04 20:15:09,106 : INFO : topic #2 (0.010): 0.188*\"food\" + 0.113*\"prime_minister\" + 0.049*\"head\" + 0.039*\"meeting\" + 0.032*\"challenge\" + 0.028*\"matter\" + 0.027*\"sector\" + 0.026*\"poor\" + 0.024*\"implementation\" + 0.023*\"meet\"\n",
      "2019-12-04 20:15:09,115 : INFO : topic #34 (0.010): 0.471*\"transaction\" + 0.030*\"customer\" + 0.025*\"authentication\" + 0.020*\"finance\" + 0.020*\"purpose\" + 0.018*\"introduce\" + 0.018*\"charge\" + 0.017*\"group\" + 0.016*\"carry\" + 0.014*\"database\"\n",
      "2019-12-04 20:15:09,123 : INFO : topic diff=0.323670, rho=0.447214\n",
      "2019-12-04 20:15:09,134 : INFO : PROGRESS: pass 0, at document #12000/13908\n",
      "2019-12-04 20:15:10,246 : INFO : merging changes from 2000 documents into a model of 13908 documents\n",
      "2019-12-04 20:15:10,257 : INFO : topic #2 (0.010): 0.170*\"food\" + 0.123*\"prime_minister\" + 0.060*\"head\" + 0.043*\"challenge\" + 0.038*\"meeting\" + 0.033*\"meet\" + 0.030*\"matter\" + 0.027*\"implementation\" + 0.023*\"sector\" + 0.023*\"poor\"\n",
      "2019-12-04 20:15:10,260 : INFO : topic #13 (0.010): 0.263*\"access\" + 0.116*\"look\" + 0.050*\"still\" + 0.038*\"demand\" + 0.032*\"recently\" + 0.031*\"able\" + 0.031*\"much\" + 0.019*\"identification\" + 0.018*\"live\" + 0.016*\"speak\"\n",
      "2019-12-04 20:15:10,266 : INFO : topic #1 (0.010): 0.278*\"amount\" + 0.089*\"poor\" + 0.071*\"step\" + 0.054*\"total\" + 0.036*\"apply\" + 0.030*\"transaction\" + 0.024*\"per_cent\" + 0.021*\"share\" + 0.014*\"introduce\" + 0.014*\"much\"\n",
      "2019-12-04 20:15:10,272 : INFO : topic #8 (0.010): 0.539*\"consumer\" + 0.045*\"amount\" + 0.021*\"customer\" + 0.021*\"continue\" + 0.013*\"june\" + 0.013*\"fail\" + 0.012*\"meeting\" + 0.011*\"charge\" + 0.011*\"total\" + 0.011*\"per_cent\"\n",
      "2019-12-04 20:15:10,287 : INFO : topic #24 (0.010): 0.143*\"privacy\" + 0.130*\"database\" + 0.113*\"authentication\" + 0.093*\"identification\" + 0.063*\"purpose\" + 0.039*\"access\" + 0.027*\"holder\" + 0.027*\"share\" + 0.025*\"obtain\" + 0.019*\"unique\"\n",
      "2019-12-04 20:15:10,291 : INFO : topic diff=0.308636, rho=0.408248\n",
      "2019-12-04 20:15:11,670 : INFO : -4.648 per-word bound, 25.1 perplexity estimate based on a held-out corpus of 1908 documents with 28559 words\n",
      "2019-12-04 20:15:11,671 : INFO : PROGRESS: pass 0, at document #13908/13908\n",
      "2019-12-04 20:15:12,659 : INFO : merging changes from 1908 documents into a model of 13908 documents\n",
      "2019-12-04 20:15:12,666 : INFO : topic #24 (0.010): 0.173*\"authentication\" + 0.127*\"database\" + 0.113*\"privacy\" + 0.089*\"identification\" + 0.065*\"purpose\" + 0.038*\"access\" + 0.030*\"holder\" + 0.026*\"share\" + 0.025*\"obtain\" + 0.020*\"unique\"\n",
      "2019-12-04 20:15:12,672 : INFO : topic #5 (0.010): 0.438*\"education\" + 0.058*\"meeting\" + 0.034*\"meet\" + 0.022*\"director\" + 0.017*\"step\" + 0.015*\"able\" + 0.013*\"begin\" + 0.013*\"other\" + 0.012*\"remain\" + 0.012*\"speak\"\n",
      "2019-12-04 20:15:12,677 : INFO : topic #7 (0.010): 0.280*\"december\" + 0.083*\"director\" + 0.067*\"special\" + 0.053*\"head\" + 0.034*\"local\" + 0.029*\"announce\" + 0.028*\"section\" + 0.025*\"charge\" + 0.021*\"wednesday\" + 0.018*\"other\"\n",
      "2019-12-04 20:15:12,689 : INFO : topic #11 (0.010): 0.254*\"election\" + 0.221*\"party\" + 0.041*\"meeting\" + 0.036*\"special\" + 0.034*\"support\" + 0.022*\"meet\" + 0.018*\"speak\" + 0.018*\"live\" + 0.016*\"prime_minister\" + 0.016*\"remain\"\n",
      "2019-12-04 20:15:12,703 : INFO : topic #32 (0.010): 0.315*\"village\" + 0.101*\"release\" + 0.096*\"administration\" + 0.074*\"local\" + 0.022*\"special\" + 0.017*\"tuesday\" + 0.016*\"other\" + 0.016*\"group\" + 0.015*\"meeting\" + 0.015*\"carry\"\n",
      "2019-12-04 20:15:12,713 : INFO : topic diff=0.305076, rho=0.377964\n",
      "2019-12-04 20:15:44,395 : INFO :  EM iter 0\n",
      "2019-12-04 20:15:44,396 : INFO : E Step\n",
      "2019-12-04 20:15:44,402 : INFO : using symmetric eta at 0.02857142857142857\n",
      "2019-12-04 20:15:44,412 : INFO : using serial LDA version on this node\n",
      "/Users/adityasenthilnathan/miniconda3/lib/python3.7/site-packages/gensim/models/ldaseqmodel.py:1474: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 394 is out of bounds for axis 1 with size 394",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-1bb321fd6ab2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mldaseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mldaseqmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaSeqModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_slice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_slices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/gensim/models/ldaseqmodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, time_slice, id2word, alphas, num_topics, initialize, sstats, lda_model, obs_variance, chain_variance, passes, random_state, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# fit DTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_lda_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlda_inference_max_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mem_min_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mem_max_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_ldaseq_ss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_chain_variance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_obs_variance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_suffstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/gensim/models/ldaseqmodel.py\u001b[0m in \u001b[0;36mfit_lda_seq\u001b[0;34m(self, corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# seq model and find the evidence lower bound. This is the E - Step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlda_seq_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_suffstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlhoods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlda_inference_max_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgammas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/gensim/models/ldaseqmodel.py\u001b[0m in \u001b[0;36mlda_seq_infer\u001b[0;34m(self, corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)\u001b[0m\n\u001b[1;32m    351\u001b[0m             bound, gammas = self.inferDTMseq(\n\u001b[1;32m    352\u001b[0m                 \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_suffstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlhoods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                 \u001b[0mldapost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlda_inference_max_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m             )\n\u001b[1;32m    355\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"DIM\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/gensim/models/ldaseqmodel.py\u001b[0m in \u001b[0;36minferDTMseq\u001b[0;34m(self, corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdoc_index\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtime_slice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                     \u001b[0mtime\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                     \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_lda_seq_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# create lda_seq slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m                     \u001b[0mdoc_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/gensim/models/ldaseqmodel.py\u001b[0m in \u001b[0;36mmake_lda_seq_slice\u001b[0;34m(self, lda, time)\u001b[0m\n\u001b[1;32m    459\u001b[0m         \"\"\"\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_chains\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me_log_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 394 is out of bounds for axis 1 with size 394"
     ]
    }
   ],
   "source": [
    "ldaseq = ldaseqmodel.LdaSeqModel(corpus=corpus, id2word=id2word, time_slice=time_slices, num_topics=35, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ldaseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
