{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b58806104929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhelper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"whitegrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import timeit\n",
    "import spacy\n",
    "import copy\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, HdpModel, LdaModel, LdaMulticore\n",
    "from nltk.corpus import stopwords\n",
    "import helper as he\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"whitegrid\") \n",
    "\n",
    "with open('../data/preprocessed_data/doc_indexes/gst.pkl','rb') as f:\n",
    "    texts,INITIAL_DOC_SIZE, DOC_TEMPORAL_INCREMENT = pickle.load(f)\n",
    "\n",
    "with open('../data/preprocessed_data/corpus_dict/gst_corp.pkl', 'rb') as f:\n",
    "    data_lemmatized, _, _ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Dictionary\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_lemmatized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dc47c23e7b8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Building Dictionary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Dictionary is built over entire set of documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_lemmatized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_lemmatized' is not defined"
     ]
    }
   ],
   "source": [
    "print('Building Dictionary')\n",
    "#Dictionary is built over entire set of documents\n",
    "id2word = Dictionary(documents=data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Golden Standard Model\n",
    "print('Building Golden Corpus')\n",
    "golden_corpus = [id2word.doc2bow(doc) for doc in data_lemmatized]\n",
    "golden_lda = LdaMulticore(golden_corpus, num_topics=35, id2word=id2word,\n",
    "                   workers=3, chunksize=2000, passes=10, batch=False)\n",
    "print('Finished Building Golden Corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positives_arr(initial_doc_size, doc_increment):\n",
    "    \n",
    "    # Set Data State to that of existing model in simulation\n",
    "    data = data_lemmatized[:initial_doc_size]\n",
    "    corpus = [id2word.doc2bow(doc) for doc in data]\n",
    "\n",
    "    print('Building Initial Model')\n",
    "    # Building for the first time - To be considered as the starting/existing model in simulation.\n",
    "    running_lda = LdaMulticore(corpus, num_topics=35, id2word=id2word,\n",
    "                       workers=3, chunksize=2000, passes=10, batch=False)\n",
    "    print('Finished Building Initial Model')\n",
    "    \n",
    "    \n",
    "    count = initial_doc_size\n",
    "    positive_arr=[]\n",
    "    no=0\n",
    "    total_len=len(data_lemmatized)\n",
    "    for i in doc_increment:\n",
    "        no+=1\n",
    "        new_docs = data_lemmatized[count:count+i]\n",
    "        count+=i\n",
    "        print('Progress upto Document no.',count,'/',total_len)\n",
    "        print('No. of New Docs:',i)\n",
    "\n",
    "        new_corp = [id2word.doc2bow(doc) for doc in new_docs]\n",
    "\n",
    "        positive_arr.append(calc_confusion_matrix(golden_lda, running_lda, new_corp))\n",
    "        \n",
    "        if(i!=doc_increment[-1]):\n",
    "            print('MODEL NO:'+str(no))\n",
    "            running_lda.update(new_corp)\n",
    "            print('MODEL DONE')\n",
    "    return positive_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_parameters(initial,k):\n",
    "    arr = []\n",
    "    arr.append(INITIAL_DOC_SIZE)\n",
    "    arr.extend(DOC_TEMPORAL_INCREMENT)\n",
    "    for i in range(1,len(arr)):\n",
    "        arr[i]=arr[i]+arr[i-1]\n",
    "        \n",
    "    doc_sizes = []\n",
    "    count=1\n",
    "    for i in range(initial+1,len(arr)):\n",
    "        if(count%k==0):\n",
    "            doc_sizes.append(arr[i])\n",
    "            count=1\n",
    "        count+=1\n",
    "        \n",
    "    doc_intervals = [arr[initial],doc_sizes[0]-arr[initial]]\n",
    "    for i in range(1,len(doc_sizes)):\n",
    "        doc_intervals.append(doc_sizes[i]-doc_sizes[i-1])\n",
    "    \n",
    "    return doc_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row_to_df(df,positive_arr,k):\n",
    "    length = len(df)\n",
    "    count=length\n",
    "    for i in positive_arr:\n",
    "        df.loc[count]=[i,count-length,k]\n",
    "        count+=1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_confusion_matrix(model1, model2, corpus1, doc_max=True):\n",
    "    lda_corpus_1 = [max(prob, key=lambda y:y[1])\n",
    "                    for prob in model1[corpus1]]\n",
    "    lda_corpus_2 = [max(prob, key=lambda y:y[1])\n",
    "                    for prob in model2[corpus1]]\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    upper_limit = len(lda_corpus_1)\n",
    "    total_permutations = upper_limit * \\\n",
    "        (upper_limit-1)/2  # nC2 combinations\n",
    "    for i in range(upper_limit):\n",
    "        for j in range(i+1, upper_limit):\n",
    "            if(lda_corpus_1[i][0] == lda_corpus_1[j][0] and lda_corpus_2[i][0] == lda_corpus_2[j][0]):\n",
    "                positive = positive+1\n",
    "            elif(lda_corpus_1[i][0] != lda_corpus_1[j][0] and lda_corpus_2[i][0] == lda_corpus_2[j][0]):\n",
    "                negative = negative+1\n",
    "            elif(lda_corpus_1[i][0] == lda_corpus_1[j][0] and lda_corpus_2[i][0] != lda_corpus_2[j][0]):\n",
    "                negative = negative+1\n",
    "            elif(lda_corpus_1[i][0] != lda_corpus_1[j][0] and lda_corpus_2[i][0] != lda_corpus_2[j][0]):\n",
    "                positive = positive+1\n",
    "    answers_positive = (round(positive*100/total_permutations, 2))\n",
    "    return answers_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('temp.pkl','wb')\n",
    "for i in range(100,101):\n",
    "# for i in range(2,3):\n",
    "    df = pd.DataFrame(columns=['positive_value','index','k'])\n",
    "    for j in range(1,2):\n",
    "#     for j in range(32,34):\n",
    "        print('Loop Parameters-',(i,j))\n",
    "        doc_params = get_doc_parameters(i,j)\n",
    "        positive = get_positives_arr(doc_params[0],doc_params[1:])\n",
    "        pickle.dump(((i,j),positive),f)\n",
    "        df = add_row_to_df(df,positive,j)\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        sns_plot = sns.violinplot(x = 'k', y = 'positive_value', data = df)\n",
    "        sns_plot = sns.pointplot(x='k', y='positive_value', data=df.groupby('k', as_index=False).median(), ax=ax, color='k', linestyle='--')\n",
    "\n",
    "        plt.savefig(\"./temp/initial_\"+str(i)+\".eps\")\n",
    "        \n",
    "        print('-----------------------------------------------')\n",
    "    df.to_csv(r'./temp/inital_'+str(i)+'.csv')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model built on 1 month data <br>\n",
    "Incremental updates done on 30*2 weeks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
